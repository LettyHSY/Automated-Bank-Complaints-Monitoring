---
title: "Embedding Predictive Models - Credit Card"
author: "Shiyi Hua"
date: "3/6/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(keras)
library(tidyverse)
library(readr)
library(tensorflow)
library(radiant)
```


## Unbalanced - Credit Card
```{r}
credit_card_total_data <- read_csv("~/git/unstructered_data/data/credit card total data.csv")

credit_card_total_data <- credit_card_total_data %>% 
  mutate(important = `Company response to consumer` == "Closed with monetary relief")

table(credit_card_total_data$important)
```


```{r}
maxWords <- 8000

tokenizer <- text_tokenizer(num_words = maxWords) %>%
  fit_text_tokenizer(credit_card_total_data$`Consumer complaint narrative`)

sequences <- texts_to_sequences(tokenizer, credit_card_total_data$`Consumer complaint narrative`)
```


```{r}
maxLength <- 500

embedding_dim_text1 <- 150

data <- pad_sequences(sequences, maxlen = maxLength)
```


```{r}
nReviews <- nrow(credit_card_total_data)

set.seed(1234)
shuffIndex <- sample(1:nReviews)

nTrain <- floor(nReviews * 0.7)
trainIndex <- shuffIndex[1:nTrain]
testIndex <- shuffIndex[(nTrain+1):nReviews]

xTrain <- data[trainIndex,]
xTest <- data[testIndex,]

y <- as.numeric(credit_card_total_data$important)
yTrain <- y[trainIndex]
yTest <- y[testIndex]
```


```{r}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = maxWords,
                  output_dim = embedding_dim_text1,
                  input_length = maxLength,
                  name = "embedding_text") %>%
  layer_flatten() %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% 
  compile(loss = "binary_crossentropy",
          optimizer = "rmsprop",
          metric = c("AUC")
  )

history <- model %>% 
  fit(x = xTrain,
      y = yTrain,
      epochs = 6,
      batch_size = 256,
      validation_split = 0.2
  )
```


```{r}
theEpoch = which.min(history$metrics$val_loss)

model <- keras_model_sequential() %>%
  layer_embedding(input_dim = maxWords,
                  output_dim = embedding_dim_text1,
                  input_length = maxLength,
                  name = "embedding_text") %>%
  layer_flatten() %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 1,activation = "sigmoid")

model %>% 
  compile(loss = "binary_crossentropy",
          optimizer = "rmsprop",
          metric = c("AUC")
  )


history <- model %>% 
  fit(x = xTrain,
      y = yTrain,
      epochs = theEpoch,
      batch_size = 256,
      validation_split = 0.0
  )


resultsEmbed <- model %>% evaluate(xTest, yTest)
resultsEmbed
```


```{r}
prediction <- model %>% predict(xTest)

xTest_credit_card_org <- credit_card_total_data[testIndex,]
xTest_credit_card_org <- cbind(xTest_credit_card_org, prediction) %>%
  select(`Consumer complaint narrative`, `Company response to consumer`, important, prediction)
```

```{r}
theTestIndices <- sample(1:10504, 4)
theTestIndices
```

```{r}
cat(str_wrap(credit_card_total_data$`Consumer complaint narrative`[testIndex[theTestIndices[1]]], width = 60))
```

```{r}
cat(str_wrap(credit_card_total_data$`Consumer complaint narrative`[testIndex[theTestIndices[2]]], width = 60))
```

```{r}
cat(str_wrap(credit_card_total_data$`Consumer complaint narrative`[testIndex[theTestIndices[3]]], width = 60))
```

```{r}
model %>% predict(xTest[theTestIndices, ])
```


```{r}
pred <- model %>% predict(xTest)

model %>% 
  compile(loss = "binary_crossentropy",
          optimizer = "rmsprop",
          metric = "accuracy"
  )


history <- model %>% 
  fit(x = xTrain,
      y = yTrain,
      epochs = theEpoch,
      batch_size = 256,
      validation_split = 0.0
  )


resultsEmbed <- model %>% evaluate(xTest, yTest)
resultsEmbed
```


## Balanced - Credit Card
```{r}
credit_card_total_data <- read_csv("~/git/unstructered_data/data/credit card total data.csv")

set.seed(1234)
reviews <- credit_card_total_data %>% 
  mutate(important = `Company response to consumer` == "Closed with monetary relief") %>%
  group_by(important) %>%
  sample_n(7186) %>%
  ungroup()

table(reviews$important)
```


```{r}
maxWords <- 10000

tokenizer <- text_tokenizer(num_words = maxWords) %>%
  fit_text_tokenizer(reviews$`Consumer complaint narrative`)

sequences <- texts_to_sequences(tokenizer, reviews$`Consumer complaint narrative`)
```


```{r}
maxLength <- 500

embedding_dim_text1 <- 150

data <- pad_sequences(sequences, maxlen = maxLength)
```


```{r}
nReviews <- nrow(reviews)

set.seed(1234)
shuffIndex <- sample(1:nReviews)

nTrain <- floor(nReviews * 0.7)
trainIndex <- shuffIndex[1:nTrain]
testIndex <- shuffIndex[(nTrain+1):nReviews]

xTrain <- data[trainIndex,]
xTest <- data[testIndex,]

y <- as.numeric(reviews$important)
yTrain <- y[trainIndex]
yTest <- y[testIndex]
```


```{r}
model <- keras_model_sequential() %>%
  layer_embedding(input_dim = maxWords,
                  output_dim = embedding_dim_text1,
                  input_length = maxLength,
                  name = "embedding_text") %>%
  layer_flatten() %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 1, activation = "sigmoid")

model %>% 
  compile(loss = "binary_crossentropy",
          optimizer = "rmsprop",
          metric="accuracy"
  )

history <- model %>% 
  fit(x = xTrain,
      y = yTrain,
      epochs = 10,
      batch_size = 256,
      validation_split = 0.2
  )
```


```{r}
theEpoch = which.min(history$metrics$val_loss)

model <- keras_model_sequential() %>%
  layer_embedding(input_dim = maxWords,
                  output_dim = embedding_dim_text1,
                  input_length = maxLength,
                  name = "embedding_text") %>%
  layer_flatten() %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 8, activation = "relu") %>%
  layer_dense(units = 1,activation = "sigmoid")

model %>% 
  compile(loss = "binary_crossentropy",
          optimizer = "rmsprop",
          metric = "accuracy"
  )


history <- model %>% 
  fit(x = xTrain,
      y = yTrain,
      epochs = theEpoch,
      batch_size = 256,
      validation_split = 0.0
  )


resultsEmbed <- model %>% evaluate(xTest, yTest)
resultsEmbed
```


```{r}
pred <- model %>% predict(xTest)

model %>% 
  compile(loss = "binary_crossentropy",
          optimizer = "rmsprop",
          metric = c("AUC")
  )


history <- model %>% 
  fit(x = xTrain,
      y = yTrain,
      epochs = theEpoch,
      batch_size = 256,
      validation_split = 0.0
  )


resultsEmbed <- model %>% evaluate(xTest, yTest)
resultsEmbed
```

